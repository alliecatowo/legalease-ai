---
title: Configuration
description: Configure LegalEase AI to match your requirements and infrastructure.
navigation:
  icon: i-lucide-settings
---

LegalEase AI is highly configurable through environment variables, configuration files, and runtime settings. This guide covers all configuration options available.

## Environment Variables

Most configuration is handled through environment variables defined in `.env` files.

### Backend Configuration

Located in `backend/.env`:

```bash
# Database
DATABASE_URL=postgresql://legalease:legalease_dev@postgres:5432/legalease
POSTGRES_DB=legalease
POSTGRES_USER=legalease
POSTGRES_PASSWORD=legalease_dev

# Redis
REDIS_URL=redis://redis:6379/0

# MinIO (S3-compatible storage)
MINIO_ENDPOINT=minio:9000
MINIO_ACCESS_KEY=legalease
MINIO_SECRET_KEY=legalease_dev_secret
MINIO_BUCKET=legalease
MINIO_SECURE=false

# Qdrant (Vector Database)
QDRANT_HOST=qdrant
QDRANT_PORT=6333
QDRANT_COLLECTION=documents

# Neo4j (Knowledge Graph)
NEO4J_URI=bolt://neo4j:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=legalease_dev

# Ollama (LLM)
OLLAMA_HOST=http://ollama:11434
OLLAMA_MODEL=llama3.1:70b

# Application
DEBUG=true
LOG_LEVEL=INFO
API_HOST=0.0.0.0
API_PORT=8000
```

### Frontend Configuration

Located in `frontend/.env`:

```bash
# Backend API
NUXT_PUBLIC_API_BASE_URL=http://localhost:8000

# Application
NUXT_PUBLIC_APP_NAME=LegalEase AI
NUXT_PUBLIC_APP_DESCRIPTION=AI-powered legal document search and analysis
```

## AI Model Configuration

Configure which AI models to use for different tasks.

### LLM Models (Ollama)

Choose models based on your hardware and requirements:

**Large Models (70B+ parameters):**
- `llama3.1:70b` - Best quality, requires 40GB+ RAM
- `mixtral:8x7b` - Good quality, mixture of experts

**Medium Models (7-13B parameters):**
- `llama3.1:8b` - Good balance of speed and quality
- `mistral:7b` - Fast and efficient

**Small Models (<7B parameters):**
- `phi3:mini` - Very fast, lower quality
- `qwen2:1.5b` - Minimal resource requirements

Configure in `backend/.env`:
```bash
OLLAMA_MODEL=llama3.1:8b
```

### Embedding Models

Choose embedding models for semantic search:

**Default (Recommended):**
- `all-MiniLM-L6-v2` - 384 dimensions, fast, good quality

**Alternatives:**
- `all-mpnet-base-v2` - 768 dimensions, slower, better quality
- `all-MiniLM-L12-v2` - 384 dimensions, balanced
- `paraphrase-multilingual-MiniLM-L12-v2` - Multilingual support

Configure in `backend/app/core/config.py`:
```python
EMBEDDING_MODEL = "all-MiniLM-L6-v2"
EMBEDDING_DIMENSIONS = 384
```

### Whisper Models

Choose Whisper model size for transcription:

**Available sizes:**
- `tiny` - Fastest, lowest accuracy (~32x real-time)
- `base` - Fast, basic accuracy (~16x real-time)
- `small` - Balanced (~10x real-time)
- `medium` - Good accuracy (~5x real-time)
- `large-v2` - Best accuracy (~2x real-time)
- `large-v3` - Latest, best accuracy

Configure in `backend/app/services/transcription.py`:
```python
WHISPER_MODEL = "medium"  # Recommended for legal use
```

## Search Configuration

Fine-tune hybrid search behavior.

### Hybrid Fusion Weights

Balance between keyword and semantic search:

```python
# backend/app/services/search.py
BM25_WEIGHT = 0.5      # Weight for keyword search
SEMANTIC_WEIGHT = 0.5  # Weight for semantic search
```

Adjust based on your needs:
- Higher BM25: More exact matching (e.g., 0.7 / 0.3)
- Higher Semantic: More conceptual matching (e.g., 0.3 / 0.7)
- Equal weights: Balanced approach (0.5 / 0.5)

### Search Result Limits

Configure how many results to return:

```python
# backend/app/api/endpoints/search.py
DEFAULT_LIMIT = 20      # Default results per query
MAX_LIMIT = 100         # Maximum allowed limit
MIN_SCORE_THRESHOLD = 0.3  # Minimum relevance score
```

### Context Windows

Control how much surrounding text to include:

```python
# backend/app/services/search.py
CONTEXT_CHARS_BEFORE = 200
CONTEXT_CHARS_AFTER = 200
```

## Document Processing

Configure document processing pipeline.

### Chunking Strategy

Control how documents are split into chunks:

```python
# backend/app/services/processing.py
CHUNK_SIZE = 512           # Characters per chunk
CHUNK_OVERLAP = 128        # Overlap between chunks
MIN_CHUNK_SIZE = 50        # Skip chunks smaller than this
MAX_CHUNKS_PER_DOC = 10000 # Limit for very large documents
```

**Guidelines:**
- Larger chunks: Better context, fewer chunks, slower search
- Smaller chunks: More granular, more chunks, faster search
- More overlap: Better coherence, more storage
- Less overlap: Less redundancy, less storage

### Processing Concurrency

Control parallel processing:

```python
# docker-compose.yml (worker service)
celery -A app.worker worker --concurrency=4
```

Adjust based on your hardware:
- CPU-only: Set to number of CPU cores
- GPU: Lower concurrency (1-2) for GPU-heavy tasks
- Mixed: Use separate queues for CPU and GPU tasks

### File Size Limits

Configure upload limits:

```python
# backend/app/api/endpoints/documents.py
MAX_FILE_SIZE_MB = 100  # Maximum file size in MB
MAX_FILES_PER_UPLOAD = 50  # Maximum files in bulk upload
```

## Transcription Configuration

Fine-tune audio transcription settings.

### Diarization

Configure speaker diarization:

```python
# backend/app/services/transcription.py
ENABLE_DIARIZATION = True
MIN_SPEAKERS = 1           # Minimum expected speakers
MAX_SPEAKERS = 10          # Maximum expected speakers
DIARIZATION_DEVICE = "cuda"  # or "cpu"
```

### VAD (Voice Activity Detection)

Configure voice activity detection:

```python
# backend/app/services/transcription.py
VAD_FILTER = True          # Remove silence
VAD_THRESHOLD = 0.5        # Sensitivity (0-1)
```

### Batch Processing

Configure batch size for transcription:

```python
# backend/app/services/transcription.py
BATCH_SIZE = 16            # Larger = faster but more memory
COMPUTE_TYPE = "float16"   # or "int8" for lower memory
```

## Database Configuration

Optimize database performance.

### Connection Pooling

Configure PostgreSQL connection pool:

```python
# backend/app/db/session.py
POOL_SIZE = 10             # Number of connections
MAX_OVERFLOW = 20          # Additional connections under load
POOL_TIMEOUT = 30          # Seconds to wait for connection
POOL_RECYCLE = 3600        # Recycle connections after 1 hour
```

### Qdrant Configuration

Configure vector database:

```python
# backend/app/core/config.py
QDRANT_COLLECTION = "documents"
QDRANT_DISTANCE = "Cosine"  # or "Euclidean", "Dot"
QDRANT_HNSW_M = 16          # HNSW parameter (higher = more accurate)
QDRANT_HNSW_EF_CONSTRUCT = 100  # Build time accuracy
```

### Neo4j Configuration

Configure graph database:

```python
# backend/app/services/knowledge_graph.py
NEO4J_MAX_CONNECTION_LIFETIME = 3600
NEO4J_MAX_CONNECTION_POOL_SIZE = 50
NEO4J_CONNECTION_TIMEOUT = 30
```

## Storage Configuration

Configure object storage and retention.

### MinIO Buckets

Organize files into buckets:

```bash
# backend/.env
MINIO_BUCKET=legalease
MINIO_DOCUMENTS_PREFIX=documents/
MINIO_AUDIO_PREFIX=audio/
MINIO_EXPORTS_PREFIX=exports/
```

### Retention Policies

Configure data retention (planned feature):

```python
# backend/app/core/config.py
DOCUMENT_RETENTION_DAYS = 365  # Keep documents for 1 year
TRANSCRIPT_RETENTION_DAYS = 365
LOG_RETENTION_DAYS = 90
EXPORT_RETENTION_DAYS = 7  # Auto-delete exports after 7 days
```

## Performance Tuning

Optimize performance for your hardware.

### GPU Configuration

Enable GPU acceleration:

**NVIDIA CUDA:**
```yaml
# docker-compose.yml
services:
  worker:
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

**AMD ROCm:**
```yaml
# docker-compose.yml
services:
  worker:
    devices:
      - /dev/kfd
      - /dev/dri
```

### Memory Limits

Set container memory limits:

```yaml
# docker-compose.yml
services:
  worker:
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G
```

### Redis Caching

Configure Redis caching:

```python
# backend/app/core/config.py
CACHE_TTL = 3600           # Cache results for 1 hour
CACHE_MAX_SIZE = 1000      # Maximum cached items
ENABLE_QUERY_CACHE = True  # Cache search queries
```

## Logging and Monitoring

Configure logging and monitoring.

### Log Levels

Set log verbosity:

```bash
# backend/.env
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL
```

### Log Format

Configure log format:

```python
# backend/app/core/logging.py
LOG_FORMAT = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
DATE_FORMAT = "%Y-%m-%d %H:%M:%S"
```

### Celery Monitoring

Monitor background tasks:

```bash
# Start Flower (Celery monitoring UI)
celery -A app.worker flower --port=5555
```

Access at: http://localhost:5555

## Security Configuration

Configure security settings (future features).

### Authentication

Enable user authentication (planned):

```python
# backend/app/core/config.py
ENABLE_AUTH = True
JWT_SECRET_KEY = "your-secret-key-here"
JWT_ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30
```

### CORS

Configure CORS for API access:

```python
# backend/app/main.py
CORS_ORIGINS = [
    "http://localhost:3000",
    "http://127.0.0.1:3000",
]
CORS_ALLOW_METHODS = ["*"]
CORS_ALLOW_HEADERS = ["*"]
```

### Rate Limiting

Configure rate limiting (planned):

```python
# backend/app/core/config.py
RATE_LIMIT_PER_MINUTE = 60
RATE_LIMIT_PER_HOUR = 1000
```

## Development vs Production

Different configurations for different environments.

### Development

```bash
# backend/.env.development
DEBUG=true
LOG_LEVEL=DEBUG
ENABLE_RELOAD=true
CORS_ORIGINS=*
```

### Production

```bash
# backend/.env.production
DEBUG=false
LOG_LEVEL=WARNING
ENABLE_RELOAD=false
CORS_ORIGINS=https://your-domain.com
```

### Docker Compose Profiles

Use profiles to switch configurations:

```bash
# Development
docker-compose --profile dev up

# Production
docker-compose --profile prod up
```

## Backup and Recovery

Configure backup strategies.

### Database Backups

Automated PostgreSQL backups:

```bash
# Backup script
pg_dump -U legalease legalease > backup_$(date +%Y%m%d).sql

# Restore
psql -U legalease legalease < backup_20241014.sql
```

### MinIO Backups

Backup object storage:

```bash
# Using mc (MinIO Client)
mc mirror legalease/legalease /backup/minio/

# Restore
mc mirror /backup/minio/ legalease/legalease
```

### Qdrant Backups

Backup vector database:

```bash
# Create snapshot
curl -X POST http://localhost:6333/collections/documents/snapshots

# Restore from snapshot
curl -X PUT http://localhost:6333/collections/documents/snapshots/upload \
  -H 'Content-Type: application/json' \
  -d '{"location": "file:///path/to/snapshot"}'
```

## Troubleshooting

Common configuration issues and solutions.

### Out of Memory

Reduce memory usage:
- Use smaller AI models
- Reduce batch sizes
- Lower concurrency
- Increase swap space
- Add memory limits to containers

### Slow Performance

Optimize performance:
- Enable GPU acceleration
- Increase worker concurrency
- Tune chunk sizes
- Enable result caching
- Optimize database indexes

### Connection Issues

Fix connectivity problems:
- Check service health: `docker-compose ps`
- Verify port availability
- Check firewall settings
- Review Docker network configuration
- Check environment variables

For more troubleshooting help, see the [Development](/docs/essentials/development) guide.
