---
title: Features
description: Deep dive into the core capabilities that ship with LegalEase today.
navigation:
  icon: i-lucide-sparkles
---

This reference outlines how the major subsystems work and what to expect from each one. Wherever functionality is experimental or pending, we call it out directly.

## Document ingestion pipeline

LegalEase processes every uploaded document through a Celery pipeline to make it searchable and easy to review.

1. **Storage** – uploads are written to MinIO under `cases/<case_id>/<uuid>_<filename>`.
2. **Parsing** – Docling extracts text, structure, tables, and bounding boxes. OCR runs automatically when the parser detects scanned pages.
3. **Chunking** – the chunker creates hierarchical slices:
   - `summary` – coarse overviews for jumping into large files
   - `section` – medium sized sections (≈500 tokens)
   - `microblock` – smaller paragraphs/snippets (≈128 tokens)
4. **Embedding** – FastEmbed produces dense vectors (default: `BAAI/bge-small-en-v1.5`), and a BM25 encoder generates sparse vectors.
5. **Indexing** – both vector types are added to Qdrant with named vectors. Chunk metadata (page, bbox, headings) is stored in PostgreSQL for the viewer.
6. **Page renders** – PDF pages are rendered to PNG for quick preview inside the dashboard.

### Document viewer

- Highlights search hits using bounding boxes captured during parsing.
- Displays per-page outlines, tables, and text layers.
- Exposes raw chunk metadata in the sidebar for debugging purposes.

## Hybrid search engine

The search service is implemented in `backend/app/services/search_service.py` and builds on Qdrant’s hybrid query API.

- **Query preparation** – BM25 sparse vector + dense embedding generated from the query text.
- **Fusion methods** – Reciprocal Rank Fusion (default) with optional DBSF support.
- **Filtering** – restrict by case IDs, document IDs, chunk types, and score thresholds.
- **Scoring metadata** – each result contains dense score, BM25 score, fusion score, and chunk provenance to help tune relevance.
- **Transcript integration** – transcript segments share the same index, letting you search documents and recordings together.

The frontend exposes a mode switch (hybrid, semantic-only, keyword-only) and presents helpful metadata so you can iteratively adjust weighting.

## Transcription pipeline

- **Pre-processing** – FFmpeg normalises bit-rate, sample rate, and channels.
- **Primary path** – WhisperX (if installed) provides transcription + alignment. Works with CUDA and ROCm GPUs; falls back to CPU when necessary.
- **Fallback path** – if WhisperX is not available, the system uses the official Whisper API via `OPENAI_API_KEY`. When no API key is set, uploads remain queued until configuration is supplied.
- **Speaker diarisation** – Pyannote is used when `HF_TOKEN` is present; otherwise a lightweight pause-based heuristic assigns speaker labels.
- **Metadata** – segments receive UUIDs, timestamps, speaker IDs, and optional word-level timings.
- **Storage** – transcripts live in PostgreSQL, originals stay in MinIO, and derived caption files are generated on demand.

### Transcript review

- Filter segments by speaker, search within a transcript, and mark key moments.
- Export DOCX/SRT/VTT/TXT/JSON.
- Monitor Celery status from the UI or via `/api/v1/transcriptions/{id}` endpoints.

### Summarisation

- `summarize_transcript` Celery task aggregates transcript text and feeds it to an Ollama model (default: `llama3.1:7b`).
- Output includes executive summaries, key moments, timelines, speaker statistics, topics, and entity sketches.
- Quick summaries (`/summary/quick`) skip the heavier analysis when you only need a synopsis.

## Forensic exports

- The scanner walks mounted evidence directories and registers folders containing both `ExportSummary.json` and `Report.html`.
- Each export is associated with a case and can be enumerated through the API (`/api/v1/forensic-exports`).
- File listings are served on demand so large exports do not need to be ingested in full.

## Local AI integration

- **Ollama** – summarisation and tagging prompts run through your local Ollama server. You can change the model via `settings.OLLAMA_MODEL_SUMMARIZATION`.
- **Model downloads** – `make setup` pulls `llama3.1` and `nomic-embed-text` by default. Add more with `make ollama-pull-<model>`.
- **Entity extraction** – GLiNER and LexNLP hooks exist but are still considered experimental; the UI does not surface results yet.

## Experimental components

These modules are present in the repository but not yet wired into the main workflows:

- **Knowledge graph service** – Neo4j schema and ingestion logic need refinement; the dashboard page still uses mocked data.
- **Analytics dashboard** – charts display placeholder values until the stats endpoints are finalised.
- **Advanced document generation** – templates/services exist for auto-generating docs but are currently stubs.

Expect rapid changes in these areas; consult the repository issues for the latest roadmap.
