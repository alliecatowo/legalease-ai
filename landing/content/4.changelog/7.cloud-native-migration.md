---
title: "Cloud-Native Architecture Migration"
description: "From self-hosted Docker stack to Firebase + frontier AI models."
date: "2025-01-15"
image: https://images.unsplash.com/photo-1451187580459-43490279c0fa?auto=format&fit=crop&w=800&q=80
---

LegalEase has undergone a significant architectural transformation, moving from a fully self-hosted Docker Compose stack to a cloud-native architecture powered by Firebase and Google Cloud services.

## Why We Migrated

The original self-hosted architecture (FastAPI, Celery, PostgreSQL, MinIO, Ollama, WhisperX) served as an excellent proof of concept, but presented challenges for production deployment:

- **Hardware requirements**: GPU-accelerated transcription required significant on-premises infrastructure
- **Model limitations**: Local models (Llama 3.1, WhisperX) couldn't match frontier model quality for legal document analysis
- **Operational complexity**: Managing 8+ Docker containers, Celery workers, and GPU drivers created operational burden
- **Scaling constraints**: Horizontal scaling required complex orchestration setup

## The New Architecture

LegalEase now runs on a modular cloud-native stack:

| Component | Before | After |
|-----------|--------|-------|
| Frontend | Nuxt 4 | Nuxt 4 (unchanged) |
| Backend API | FastAPI + Celery | Firebase Cloud Functions + Genkit |
| Database | PostgreSQL | Cloud Firestore |
| File Storage | MinIO | Firebase Storage |
| Transcription | WhisperX / Whisper API | Gemini 2.5 Flash / Google Speech-to-Text (Chirp 3) |
| Summarization | Ollama (Llama 3.1) | Gemini 2.5 Flash |
| Vector Search | Qdrant (self-hosted) | Qdrant Cloud |
| Auth | Stub/None | Firebase Authentication |

## Key Benefits

**Frontier Model Quality**
- Gemini 2.5 Flash provides superior transcription with automatic speaker name inference
- Better summarization, entity extraction, and key moment identification
- Multi-modal capabilities for future document analysis features

**Simplified Setup**
- Local development requires only a Gemini API key and Firebase emulators
- No GPU required for development or testing
- Single `mise run dev:local` command starts everything

**Multi-Agent Ready**
- Genkit-based flows support future multi-agent orchestration
- Provider abstraction allows swapping AI backends without code changes
- Ready for Claude, GPT-4, or other frontier models

**Still Self-Hostable**
- Firebase emulators provide full offline development capability
- Qdrant can run locally or in cloud
- Architecture designed for eventual Kubernetes and AWS deployment options

## Future Roadmap

We're committed to avoiding vendor lock-in:

- **Kubernetes deployment**: Helm charts for self-hosted production deployments
- **AWS alternative**: S3, DynamoDB, and Lambda equivalents planned
- **Provider flexibility**: Additional transcription/AI providers (OpenAI, Anthropic) in progress

The modular provider pattern means you can mix and match services based on your requirements, compliance needs, or cost constraints.

## Migration Notes

If you were running the previous Docker-based LegalEase:

1. Export your PostgreSQL data before migrating
2. The new architecture uses different data models optimized for Firestore
3. Transcripts and documents need to be re-processed through the new pipeline
4. User authentication is now handled by Firebase Auth (Google sign-in supported)

The landing documentation has been fully updated to reflect the new architecture. See the [Installation Guide](/docs/getting-started/installation) for the new setup process.
